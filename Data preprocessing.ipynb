{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6cc8d5",
   "metadata": {},
   "source": [
    "__import re__\n",
    "\n",
    "* Used for string cleaning and normalization.\n",
    "\n",
    "\n",
    "__import csv__\n",
    "\n",
    "* Allows reading and writing CSV files in plain Python.\n",
    "* Save cleaned transactions row by row or read CSVs without pandas.\n",
    "\n",
    "__import os__\n",
    "\n",
    "* Provides tools to interact with the filesystem.\n",
    "\n",
    "_Used for_\n",
    "\n",
    "* Creating directories (os.makedirs)\n",
    "\n",
    "* Joining file paths (os.path.join)\n",
    "\n",
    "* Checking if directories exist\n",
    "\n",
    "__from collections import Counter, defaultdict__\n",
    "\n",
    "* Counts frequency of items in a list or iterable.\n",
    "\n",
    "__defaultdict:__\n",
    "\n",
    "* Like a normal dictionary but provides default values automatically for missing keys.\n",
    "\n",
    "__from typing import List, Tuple, Dict, Any, Optional__\n",
    "\n",
    "* Provides type hints to make code more readable and help with debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "07a0182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                      \n",
    "import csv                      \n",
    "import os                      \n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import pandas as pd           \n",
    "from mlxtend.preprocessing import TransactionEncoder  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544638ee",
   "metadata": {},
   "source": [
    "This block sets all paths, parameters, and directories needed for the preprocessing pipeline so that your code can read the \n",
    "\n",
    "raw CSV, clean it, and save outputs safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7cc234",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_DIR = \"preprocessed_outputs\"\n",
    "# Use the local CSV in the repository by default. Change this to an absolute path if needed.\n",
    "LOAD_PATH = \"supermarket_transactions.csv\"\n",
    "MIN_ITEMS_PER_TX = 2\n",
    "MAX_ITEMS_PER_TX = 7\n",
    "OHE_CSV_NAME = \"one_hot_transactions.csv\"\n",
    "CLEAN_CSV_NAME = \"clean_transactions.csv\"\n",
    "SUMMARY_CSV_NAME = \"preprocessing_summary.csv\"\n",
    "\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb51d5",
   "metadata": {},
   "source": [
    "_str(item)_  : ensures the input is a string (handles None or numbers).\n",
    "\n",
    "_.strip()_  : removes spaces at the beginning or end (\" Milk \" → \"Milk\").\n",
    "\n",
    "_.lower()_ :  everything to lowercase (\"Milk\" → \"milk\").\n",
    "\n",
    "_re.sub(pattern, replacement, string)_ : replaces all characters matching the pattern with a space.\n",
    "\n",
    "_\\s+_ : matches one or more whitespace characters\n",
    "\n",
    "Replaces them with a single space\n",
    "\n",
    "_s.replace_ : \"&\" to \" and \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "869409bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_item_name(item: str) -> str:\n",
    "    s = str(item).strip().lower()\n",
    "    s = re.sub(r\"[\\.\\-_/\\\\\\(\\)]\", \" \", s)  # remove punctuation\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adfe736",
   "metadata": {},
   "source": [
    "__Example use__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f49d3a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['Fish', 'Salt', 'Sugar', 'Cereal', 'Soap', 'Toothpaste']\n",
      "Cleaned : ['fish', 'salt', 'sugar', 'cereal', 'soap', 'toothpaste']\n",
      "\n",
      "Original: ['Shampoo', 'Oranges', 'Potatoes']\n",
      "Cleaned : ['shampoo', 'oranges', 'potatoes']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions = [\n",
    "    [\"Fish\", \"Salt\", \"Sugar\", \"Cereal\", \"Soap\", \"Toothpaste\"],\n",
    "    [\"Shampoo\", \"Oranges\", \"Potatoes\"]\n",
    "]\n",
    "for tx in transactions:\n",
    "    cleaned_tx = [normalize_item_name(item) for item in tx]\n",
    "    print(f\"Original: {tx}\")\n",
    "    print(f\"Cleaned : {cleaned_tx}\")\n",
    "    print(\"\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fce2b2",
   "metadata": {},
   "source": [
    "This ensures no empty items or None values remain in a single transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "238f868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_transaction(tx: List[str]) -> List[str]:\n",
    "    cleaned = [normalize_item_name(it) for it in tx if it and normalize_item_name(it) != \"\"]\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82952c9",
   "metadata": {},
   "source": [
    "_min items_ / _max items_: filters out transactions that are beyond the 2 or 7 boundary\n",
    "\n",
    "transactions outside the allowed item range are dropped.\n",
    "\n",
    "only transactions with an acceptable number of items are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "67096fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_transactions(transactions: List[List[str]],\n",
    "                       min_items: int = MIN_ITEMS_PER_TX,\n",
    "                       max_items: int = MAX_ITEMS_PER_TX) -> List[List[str]]:\n",
    "    cleaned_all = []\n",
    "    for tx in transactions:\n",
    "        cleaned_tx = clean_transaction(tx)\n",
    "        if len(cleaned_tx) < min_items:\n",
    "            continue\n",
    "        if len(cleaned_tx) > max_items:\n",
    "            continue\n",
    "        cleaned_all.append(cleaned_tx)\n",
    "    return cleaned_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a30dc",
   "metadata": {},
   "source": [
    "__Sorts the items in alphabetical order:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "167128ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_items_in_transactions(transactions: List[List[str]]) -> List[List[str]]:\n",
    "    return [sorted(tx) for tx in transactions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "25109f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bread', 'Eggs', 'Milk']\n",
      "['Juice', 'Soap', 'Tea']\n",
      "['Coffee', 'Onions', 'Potatoes']\n"
     ]
    }
   ],
   "source": [
    "transactions = [\n",
    "    [\"Milk\", \"Bread\", \"Eggs\"],\n",
    "    [\"Juice\", \"Soap\", \"Tea\"],\n",
    "    [\"Potatoes\", \"Coffee\", \"Onions\"]\n",
    "]\n",
    "\n",
    "sorted_tx = sort_items_in_transactions(transactions)\n",
    "for tx in sorted_tx:\n",
    "    print(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe69f5b",
   "metadata": {},
   "source": [
    "`TransactionEncoder()` comes from mlxtend.\n",
    "\n",
    "It converts transactions into a boolean array  for each unique item.\n",
    "\n",
    "`fit(transactions)` finds all unique items in your dataset.\n",
    "\n",
    "`transform(transactions)` creates a 2D array where:\n",
    "\n",
    "Rows = transactions\n",
    "\n",
    "Columns = unique items\n",
    "\n",
    "Value = True if the item exists in that transaction, else False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2c21dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_transactions(transactions: List[List[str]]) -> pd.DataFrame:\n",
    "    \"\"\"One-hot encode transactions for Apriori.\"\"\"\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c798fb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bread  coffee  eggs  juice  milk  onions  potatoes  soap\n",
      "0      1       0     1      0     1       0         0     0\n",
      "1      0       0     0      1     0       0         0     1\n",
      "2      0       1     0      0     0       1         1     0\n"
     ]
    }
   ],
   "source": [
    "transactions = [\n",
    "    ['milk', 'bread', 'eggs'],\n",
    "    ['juice', 'soap'],\n",
    "    ['potatoes', 'coffee', 'onions']\n",
    "]\n",
    "ohe_df = one_hot_encode_transactions(transactions)\n",
    "print(ohe_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84629399",
   "metadata": {},
   "source": [
    "__1 → the transaction contains that item.__\n",
    "\n",
    "__0 → the transaction does not contain that item__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "202031a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_transactions': 3,\n",
       " 'avg_tx_length': 2.6666666666666665,\n",
       " 'min_tx_length': 2,\n",
       " 'max_tx_length': 3,\n",
       " 'num_unique_items': 8,\n",
       " 'unique_items_sample': ['bread',\n",
       "  'coffee',\n",
       "  'eggs',\n",
       "  'juice',\n",
       "  'milk',\n",
       "  'onions',\n",
       "  'potatoes',\n",
       "  'soap'],\n",
       " 'top_20_items': [('milk', 1),\n",
       "  ('bread', 1),\n",
       "  ('eggs', 1),\n",
       "  ('juice', 1),\n",
       "  ('soap', 1),\n",
       "  ('potatoes', 1),\n",
       "  ('coffee', 1),\n",
       "  ('onions', 1)],\n",
       " 'item_counts': {'milk': 1,\n",
       "  'bread': 1,\n",
       "  'eggs': 1,\n",
       "  'juice': 1,\n",
       "  'soap': 1,\n",
       "  'potatoes': 1,\n",
       "  'coffee': 1,\n",
       "  'onions': 1}}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_data_quality(transactions: List[List[str]]) -> Dict[str, Any]:\n",
    "    n = len(transactions)\n",
    "    lengths = [len(tx) for tx in transactions]\n",
    "    avg_len = sum(lengths)/n if n else 0\n",
    "    min_len = min(lengths) if lengths else 0\n",
    "    max_len = max(lengths) if lengths else 0\n",
    "    flat_items = [item for tx in transactions for item in tx]\n",
    "    unique_items = sorted(set(flat_items))\n",
    "    item_counts = Counter(flat_items)\n",
    "    summary = {\n",
    "        \"num_transactions\": n,\n",
    "        \"avg_tx_length\": avg_len,\n",
    "        \"min_tx_length\": min_len,\n",
    "        \"max_tx_length\": max_len,\n",
    "        \"num_unique_items\": len(unique_items),\n",
    "        \"unique_items_sample\": unique_items[:30],\n",
    "        \"top_20_items\": item_counts.most_common(20),\n",
    "        \"item_counts\": dict(item_counts)\n",
    "    }\n",
    "    return summary\n",
    "summary_stats = check_data_quality(transactions)\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e3703039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_transactions_csv(df: pd.DataFrame, export_dir: str, filename: str):\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    path = os.path.join(export_dir, filename)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Cleaned transactions CSV saved to: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6c42aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ohe_csv(ohe_df: pd.DataFrame, export_dir: str, filename: str):\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    path = os.path.join(export_dir, filename)\n",
    "    ohe_df.to_csv(path, index=False)\n",
    "    print(f\"One-hot encoded CSV saved to: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a415773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summary_csv(summary: Dict[str, Any], export_dir: str, filename: str):\n",
    "    \"\"\"Save a human-readable summary DataFrame built from the summary dict.\"\"\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    # Build a flat summary dataframe\n",
    "    flat = {\n",
    "        \"num_transactions\": summary.get(\"num_transactions\", 0),\n",
    "        \"avg_tx_length\": summary.get(\"avg_tx_length\", 0),\n",
    "        \"min_tx_length\": summary.get(\"min_tx_length\", 0),\n",
    "        \"max_tx_length\": summary.get(\"max_tx_length\", 0),\n",
    "        \"num_unique_items\": summary.get(\"num_unique_items\", 0)\n",
    "    }\n",
    "    df_flat = pd.DataFrame([flat])\n",
    "    # Top items (if present) as separate columns concatenated\n",
    "    top_items = summary.get(\"top_20_items\", [])\n",
    "    if top_items:\n",
    "        df_top = pd.DataFrame(top_items, columns=[\"item\", \"count\"])\n",
    "        df_summary = pd.concat([df_flat, df_top], axis=1)\n",
    "    else:\n",
    "        df_summary = df_flat\n",
    "    path = os.path.join(export_dir, filename)\n",
    "    df_summary.to_csv(path, index=False)\n",
    "    print(f\"Summary CSV saved to: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "923ebb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = {\n",
    "        \"num_transactions\": summary_stats.get(\"num_transactions\", 0),\n",
    "        \"avg_tx_length\": summary_stats.get(\"avg_tx_length\", 0),\n",
    "        \"min_tx_length\": summary_stats.get(\"min_tx_length\", 0),\n",
    "        \"max_tx_length\": summary_stats.get(\"max_tx_length\", 0),\n",
    "        \"num_unique_items\": summary_stats.get(\"num_unique_items\", 0)\n",
    "    }\n",
    "df_flat = pd.DataFrame([flat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0555541f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_transactions</th>\n",
       "      <th>avg_tx_length</th>\n",
       "      <th>min_tx_length</th>\n",
       "      <th>max_tx_length</th>\n",
       "      <th>num_unique_items</th>\n",
       "      <th>item</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>milk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bread</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eggs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>juice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>soap</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>potatoes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coffee</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>onions</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_transactions  avg_tx_length  min_tx_length  max_tx_length  \\\n",
       "0               3.0       2.666667            2.0            3.0   \n",
       "1               NaN            NaN            NaN            NaN   \n",
       "2               NaN            NaN            NaN            NaN   \n",
       "3               NaN            NaN            NaN            NaN   \n",
       "4               NaN            NaN            NaN            NaN   \n",
       "5               NaN            NaN            NaN            NaN   \n",
       "6               NaN            NaN            NaN            NaN   \n",
       "7               NaN            NaN            NaN            NaN   \n",
       "\n",
       "   num_unique_items      item  count  \n",
       "0               8.0      milk      1  \n",
       "1               NaN     bread      1  \n",
       "2               NaN      eggs      1  \n",
       "3               NaN     juice      1  \n",
       "4               NaN      soap      1  \n",
       "5               NaN  potatoes      1  \n",
       "6               NaN    coffee      1  \n",
       "7               NaN    onions      1  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and display a readable summary DataFrame if `summary_stats` exists\n",
    "try:\n",
    "    flat = {\n",
    "        \"num_transactions\": summary_stats.get(\"num_transactions\", 0),\n",
    "        \"avg_tx_length\": summary_stats.get(\"avg_tx_length\", 0),\n",
    "        \"min_tx_length\": summary_stats.get(\"min_tx_length\", 0),\n",
    "        \"max_tx_length\": summary_stats.get(\"max_tx_length\", 0),\n",
    "        \"num_unique_items\": summary_stats.get(\"num_unique_items\", 0)\n",
    "    }\n",
    "    df_flat = pd.DataFrame([flat])\n",
    "    top_items = summary_stats.get(\"top_20_items\", [])\n",
    "    if top_items:\n",
    "        df_top = pd.DataFrame(top_items, columns=[\"item\", \"count\"])\n",
    "        df_summary = pd.concat([df_flat, df_top], axis=1)\n",
    "    else:\n",
    "        df_summary = df_flat\n",
    "    display(df_summary)\n",
    "except NameError:\n",
    "    print(\"summary_stats is not defined. Run the preprocessing pipeline first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f0d3a",
   "metadata": {},
   "source": [
    "`num_transactions`\tTotal number of transactions in your dataset. Only the first row has this value because the code repeated the summary stats for every top item. Here, 5000 transactions.\n",
    "\n",
    "`avg_tx_length`\tAverage number of items per transaction. Here 4.4904 items on average.\n",
    "\n",
    "`min_tx_length`\tSmallest transaction length (number of items). Here 2.0 items. \n",
    "`max_tx_length`\tLargest transaction length. Here 7.0 items. \n",
    "\n",
    "`num_unique_items`\tTotal number of unique items across all transactions. Here 30. \n",
    "\n",
    "`item`\t_Name of the item_. This comes from the top 20 most frequent items list. Repeats down the rows.\n",
    "count\tHow many times that item appears across all transactions. Example: \"soap\" occurs 788 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ba4db49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(LOAD_PATH: Optional[str] = None) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n",
    "    df_original = pd.read_csv(LOAD_PATH) \n",
    "    transactions_raw = df_original.iloc[:, -1].apply(lambda x: str(x).split(\",\")).tolist()\n",
    "\n",
    "    cleaned_tx = clean_transactions(transactions_raw) \n",
    "    sorted_tx = sort_items_in_transactions(cleaned_tx)\n",
    "\n",
    "    df_cleaned = pd.DataFrame({\"cleaned_items\": [\"; \".join(tx) for tx in sorted_tx]})\n",
    "    \n",
    "    ohe_df = one_hot_encode_transactions(sorted_tx)\n",
    "    summary_stats = check_data_quality(sorted_tx)\n",
    "\n",
    "    return df_cleaned, ohe_df, summary_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d55a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned transactions CSV saved to: preprocessed_outputs\\clean_transactions.csv\n",
      "One-hot encoded CSV saved to: preprocessed_outputs\\one_hot_transactions.csv\n",
      "Summary CSV saved to: preprocessed_outputs\\preprocessing_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure preprocessing has been executed before saving outputs\n",
    "try:\n",
    "    df_cleaned, ohe_df, summary_stats  # check if already present\n",
    "except NameError:\n",
    "    df_cleaned, ohe_df, summary_stats = preprocess_pipeline(LOAD_PATH)\n",
    "\n",
    "save_clean_transactions_csv(df_cleaned, EXPORT_DIR, CLEAN_CSV_NAME)\n",
    "save_ohe_csv(ohe_df, EXPORT_DIR, OHE_CSV_NAME)\n",
    "save_summary_csv(summary_stats, EXPORT_DIR, SUMMARY_CSV_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6c3ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/USER/Downloads/Data Warehousing/supermarket_transactions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[177]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     df_cleaned, df_ohe, summary_stats = \u001b[43mpreprocess_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLOAD_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of transactions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary_stats[\u001b[33m'\u001b[39m\u001b[33mnum_transactions\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnique items (sample): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary_stats[\u001b[33m'\u001b[39m\u001b[33munique_items_sample\u001b[39m\u001b[33m'\u001b[39m][:\u001b[32m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[175]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mpreprocess_pipeline\u001b[39m\u001b[34m(LOAD_PATH)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_pipeline\u001b[39m(LOAD_PATH: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     df_original = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLOAD_PATH\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m      3\u001b[39m     transactions_raw = df_original.iloc[:, -\u001b[32m1\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mstr\u001b[39m(x).split(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)).tolist()\n\u001b[32m      5\u001b[39m     cleaned_tx = clean_transactions(transactions_raw) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:/Users/USER/Downloads/Data Warehousing/supermarket_transactions.csv'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_cleaned, ohe_df, summary_stats = preprocess_pipeline(LOAD_PATH)\n",
    "    \n",
    "    print(f\"Number of transactions: {summary_stats['num_transactions']}\")\n",
    "    print(f\"Unique items (sample): {summary_stats['unique_items_sample'][:10]}\")\n",
    "    print(\"Top 10 items (item, count):\")\n",
    "    print(summary_stats[\"top_20_items\"][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori: generate frequent itemsets and export top 10\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import os\n",
    "\n",
    "# Ensure the preprocessing pipeline has been run and `ohe_df` exists.\n",
    "try:\n",
    "    ohe_df  # noqa: F821\n",
    "except NameError:\n",
    "    df_cleaned, ohe_df, summary_stats = preprocess_pipeline(LOAD_PATH)\n",
    "\n",
    "# Run apriori to get frequent itemsets with minimum support 0.05\n",
    "frequent_itemsets = apriori(ohe_df, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Add readable itemset string for display/export\n",
    "frequent_itemsets['itemset_str'] = frequent_itemsets['itemsets'].apply(lambda s: ', '.join(sorted(list(s))))\n",
    "\n",
    "# Sort by support and select top 10\n",
    "frequent_itemsets_sorted = frequent_itemsets.sort_values(by='support', ascending=False).reset_index(drop=True)\n",
    "top10_itemsets = frequent_itemsets_sorted.head(10)\n",
    "\n",
    "# Display top 10 itemsets (itemset string and support)\n",
    "print(f\"Total frequent itemsets found: {len(frequent_itemsets)}\")\n",
    "display(top10_itemsets[['itemset_str', 'support']])\n",
    "\n",
    "# Export the top 10 itemsets to CSV\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "export_path = os.path.join(EXPORT_DIR, 'top10_itemsets.csv')\n",
    "top10_itemsets[['itemset_str', 'support']].to_csv(export_path, index=False)\n",
    "print(f\"Top 10 itemsets exported to: {export_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
