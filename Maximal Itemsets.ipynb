{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f101891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.23.4)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlxtend) (1.16.3)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlxtend) (2.3.4)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlxtend) (2.3.3)\n",
      "Requirement already satisfied: scikit-learn>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlxtend) (1.7.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlxtend) (3.10.7)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib>=3.0.0->mlxtend) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install mlxtend\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0388937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Student:Allan)\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "df = pd.read_csv(r'C:\\Users\\user\\supermarket_transactions.csv') # Uses pandas to read the CSV file from the specified path.\n",
    "                                 # The 'r' prefix makes it a raw string, correctly handling the backslashes in Windows paths.\n",
    "                                 # The file contains columns like TransactionID, CustomerName, Date, and Items.\n",
    "                                 # The 'Items' column holds a string of comma-separated items for each transaction.\n",
    "df['Items'] = df['Items'].str.split(',') # Accesses the 'Items' column of the DataFrame.\n",
    "                                 # The '.str' accessor allows string operations on the entire column.\n",
    "                                 # '.split(',')' splits each string value in the column by the comma character.\n",
    "                                 # This converts each row's 'Items' value from a string like \"A,B,C\" into a list like ['A', 'B', 'C'].\n",
    "                                 # This is the format required by TransactionEncoder.\n",
    "\n",
    "# Extract the list of itemsets (transactions)\n",
    "transaction_list = df['Items'].tolist() # Converts the 'Items' column (now containing lists) from the DataFrame\n",
    "                                 # into a standard Python list of lists. Each inner list represents one transaction's items.\n",
    "encoder = TransactionEncoder()    # Creates an instance of the TransactionEncoder class.\n",
    "                                 # This object will be used to fit the transaction data and then transform it.\n",
    "one_hot_encoded_matrix = encoder.fit(transaction_list).transform(transaction_list)\n",
    "                                 # First, '.fit(transaction_list)' analyzes the list of itemsets to identify all unique items\n",
    "                                 # and their potential presence across transactions.\n",
    "                                 # Then, '.transform(transaction_list)' converts the list of itemsets into a binary matrix.\n",
    "                                 # Each row in the matrix corresponds to a transaction.\n",
    "                                 # Each column corresponds to a unique item.\n",
    "                                 # A cell contains True if the item was present in that transaction, False otherwise.\n",
    "df_encoded = pd.DataFrame(one_hot_encoded_matrix, columns=encoder.columns_)\n",
    "                                 # Converts the resulting binary numpy matrix into a pandas DataFrame.\n",
    "                                 # 'encoder.columns_' provides the column names (the unique items found during fitting).\n",
    "                                 # This DataFrame 'df_encoded' is the final input format required by the Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff3b7bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frequent itemsets (support >= 0.01): 465\n",
      "\n",
      "First few frequent itemsets found by Apriori:\n",
      "   support   itemsets\n",
      "0   0.1484   (Apples)\n",
      "1   0.1552  (Bananas)\n",
      "2   0.1440     (Beef)\n",
      "3   0.1574    (Bread)\n",
      "4   0.1450   (Butter)\n",
      "5   0.1466  (Carrots)\n",
      "6   0.1478   (Cereal)\n",
      "7   0.1460   (Cheese)\n",
      "8   0.1486  (Chicken)\n",
      "9   0.1524   (Coffee)\n"
     ]
    }
   ],
   "source": [
    "# --- (Student:Allan) 2. Find Frequent Itemsets using Apriori ---\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "min_support_threshold = 0.01     # Defines the minimum support threshold as a decimal (0.01 = 1%).\n",
    "                                 # An itemset must appear in at least 1% of all transactions to be considered 'frequent'.\n",
    "frequent_itemsets = apriori(df_encoded, min_support=min_support_threshold, use_colnames=True)\n",
    "                                 # Calls the Apriori algorithm from mlxtend.\n",
    "                                 # 'df_encoded': The one-hot encoded DataFrame is passed as the dataset.\n",
    "                                 # 'min_support=0.01': Sets the minimum support threshold.\n",
    "                                 # 'use_colnames=True': Ensures the output DataFrame uses the actual item names\n",
    "                                 # (e.g., 'Bread', 'Milk') as the elements in the itemsets, instead of column indices.\n",
    "                                 # The result is a pandas DataFrame with columns 'support' and 'itemsets'.\n",
    "                                 # 'support' contains the support value for each frequent itemset.\n",
    "                                 # 'itemsets' contains the actual set of items (as a frozenset if use_colnames=True).\n",
    "\n",
    "# Display the total number of frequent itemsets found\n",
    "print(f\"Total number of frequent itemsets (support >= {min_support_threshold}): {len(frequent_itemsets)}\")\n",
    "                                 # Prints the total count of itemsets found by Apriori that meet the minimum support.\n",
    "print(\"\\nFirst few frequent itemsets found by Apriori:\")\n",
    "                                 # Prints a header for the next output.\n",
    "print(frequent_itemsets.head(10)) # Prints the first 10 rows of the 'frequent_itemsets' DataFrame to preview results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38013906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (Student:Allan) 3. Identify Maximal Frequent Itemsets ---\n",
    "# Convert the 'itemsets' column of the frequent_itemsets DataFrame into a Python set of frozensets.\n",
    "frequent_itemsets_set = set(frequent_itemsets['itemsets'].apply(frozenset))\n",
    "                                 # Creates a Python 'set' containing all the frequent itemsets found by Apriori.\n",
    "                                 # 'frequent_itemsets['itemsets']' accesses the column of itemsets.\n",
    "                                 # '.apply(frozenset)' ensures each itemset is a frozenset (immutable set).\n",
    "                                 # Using a 'set' allows for very fast O(1) average time complexity\n",
    "                                 # when checking if a specific itemset exists within it later.\n",
    "                                 # This is crucial for the efficiency of the maximal check loop.\n",
    "\n",
    "# Iterate through each frequent itemset found by Apriori\n",
    "maximal_itemsets = []            # Initializes an empty list to store the itemsets that are determined to be maximal.\n",
    "for itemset in frequent_itemsets_set: # Loops through every single frequent itemset found by Apriori.\n",
    "    is_maximal = True            # Assumes the current 'itemset' is maximal until proven otherwise.\n",
    "    for item in df_encoded.columns: # Loops through every unique item present in the entire dataset\n",
    "                                 # (as represented by the columns of the encoded DataFrame).\n",
    "        # Form a potential superset by adding the item\n",
    "        if item not in itemset:  # Checks if the current 'item' from the dataset is NOT already in the 'itemset' being checked.\n",
    "            potential_superset = frozenset(itemset | {item}) # Creates a new frozenset by taking the current 'itemset'\n",
    "                                 # and adding the current 'item' to it. This new set is a potential superset of 'itemset'.\n",
    "            if potential_superset in frequent_itemsets_set: # Checks if this potential superset exists in our collection\n",
    "                                 # of *all* frequent itemsets found previously by Apriori.\n",
    "                is_maximal = False # If the potential superset is frequent, then the original 'itemset' cannot be maximal.\n",
    "                break            # Exits the inner loop over items, as we've already proven it's not maximal.\n",
    "\n",
    "    # After checking all possible items for the current itemset,\n",
    "    # if 'is_maximal' is still True, it means no frequent superset was found.\n",
    "    if is_maximal:               # If the loop over all items completed without finding a frequent superset...\n",
    "        maximal_itemsets.append(itemset) # ...then the 'itemset' is maximal, and it's added to the list.\n",
    "\n",
    "# Create a DataFrame with the 'itemsets' column containing the maximal itemsets\n",
    "maximal_df = pd.DataFrame({'itemsets': maximal_itemsets})\n",
    "                                 # Converts the list of maximal itemsets (frozensets) into a new pandas DataFrame.\n",
    "                                 # This creates the structure for the final output, similar to the Apriori result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8b49e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (Student:Allan) 4. Retrieving Support Values for Maximal Itemsets ---\n",
    "# Initialize an empty list for the support values of maximal itemsets\n",
    "maximal_supports = []            # Initializes an empty list to store the support values corresponding to the maximal itemsets.\n",
    "\n",
    "for idx, row in maximal_df.iterrows(): # Iterates through each row of the 'maximal_df' DataFrame.\n",
    "    max_itemset_frozenset = row['itemsets'] # Gets the frozenset itemset from the current row.\n",
    "    support_value = frequent_itemsets[frequent_itemsets['itemsets'].apply(frozenset) == max_itemset_frozenset]['support'].iloc[0]\n",
    "                                 # Finds the corresponding support value for the current maximal itemset.\n",
    "                                 # It does this by searching the original 'frequent_itemsets' DataFrame.\n",
    "                                 # It filters rows where the 'itemsets' column (converted to frozenset for comparison)\n",
    "                                 # matches the 'max_itemset_frozenset'.\n",
    "                                 # The ['support'] accesses the support column of the filtered result.\n",
    "                                 # '.iloc[0]' gets the first (and should be only) value from the support series.\n",
    "    maximal_supports.append(support_value) # Adds the found support value to the list.\n",
    "\n",
    "# Add the calculated support values as a new column to the maximal DataFrame\n",
    "maximal_df['support'] = maximal_supports\n",
    "                                 # Adds the list of support values as a new column named 'support' to the 'maximal_df'.\n",
    "                                 # Now, 'maximal_df' has both the itemsets and their corresponding support values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61bade8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total number of MAXIMAL frequent itemsets (support >= 0.01): 435\n",
      "\n",
      "Maximal Frequent Itemsets:\n",
      "               itemsets  support\n",
      "0        (Butter, Salt)   0.0206\n",
      "1       (Bread, Yogurt)   0.0206\n",
      "2       (Salt, Bananas)   0.0226\n",
      "3       (Bread, Grapes)   0.0176\n",
      "4       (Sugar, Cereal)   0.0214\n",
      "..                  ...      ...\n",
      "430     (Bread, Cereal)   0.0202\n",
      "431  (Coffee, Potatoes)   0.0220\n",
      "432       (Sugar, Milk)   0.0226\n",
      "433   (Grapes, Chicken)   0.0218\n",
      "434   (Juice, Tomatoes)   0.0214\n",
      "\n",
      "[435 rows x 2 columns]\n",
      "\n",
      "The longest maximal itemset found contains 2 items.\n"
     ]
    }
   ],
   "source": [
    "# --- (Student:Allan) 5. Output the Results ---\n",
    "print(f\"\\n\\nTotal number of MAXIMAL frequent itemsets (support >= {min_support_threshold}): {len(maximal_df)}\")\n",
    "                                 # Prints the total count of itemsets found to be maximal.\n",
    "print(\"\\nMaximal Frequent Itemsets:\")\n",
    "                                 # Prints a header for the final output.\n",
    "print(maximal_df)                # Prints the entire DataFrame containing the maximal itemsets and their supports.\n",
    "\n",
    "# Print the length (number of items) of the largest maximal itemset found\n",
    "if not maximal_df.empty:         # Checks if the maximal DataFrame is not empty (i.e., any maximal itemsets were found).\n",
    "    max_length = maximal_df['itemsets'].apply(len).max() # Calculates the length of each itemset in the 'itemsets' column\n",
    "                                 # and finds the maximum length among them.\n",
    "    print(f\"\\nThe longest maximal itemset found contains {max_length} items.\")\n",
    "                                 # Prints the maximum length found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bd56601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximal frequent itemsets have been saved to 'C:\\Users\\user\\maximal_frequent_itemsets.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Save the Results to CSV ---\n",
    "# [Student: Allan] Save the maximal frequent itemsets DataFrame as a CSV file to the specified directory\n",
    "maximal_df.to_csv(r'C:\\Users\\user\\maximal_frequent_itemsets.csv', index=False)\n",
    "# The 'index=False' parameter ensures that the row indices are not saved as a separate column in the CSV file.\n",
    "# The raw string (r\"\") is used again to correctly handle the Windows file path.\n",
    "print(f\"\\nMaximal frequent itemsets have been saved to 'C:\\\\Users\\\\user\\\\maximal_frequent_itemsets.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be91b0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frequent itemsets (support >= 0.01): 465\n",
      "\n",
      "First few frequent itemsets found by Apriori:\n",
      "   support   itemsets\n",
      "0   0.1484   (Apples)\n",
      "1   0.1552  (Bananas)\n",
      "2   0.1440     (Beef)\n",
      "3   0.1574    (Bread)\n",
      "4   0.1450   (Butter)\n",
      "5   0.1466  (Carrots)\n",
      "6   0.1478   (Cereal)\n",
      "7   0.1460   (Cheese)\n",
      "8   0.1486  (Chicken)\n",
      "9   0.1524   (Coffee)\n"
     ]
    }
   ],
   "source": [
    "# --- (Student:Allan) 2. Find Frequent Itemsets using Apriori ---\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "min_support_threshold = 0.01     # Defines the minimum support threshold as a decimal (0.01 = 1%).\n",
    "                                 # An itemset must appear in at least 1% of all transactions to be considered 'frequent'.\n",
    "frequent_itemsets = apriori(df_encoded, min_support=min_support_threshold, use_colnames=True)\n",
    "                                 # Calls the Apriori algorithm from mlxtend.\n",
    "                                 # 'df_encoded': The one-hot encoded DataFrame is passed as the dataset.\n",
    "                                 # 'min_support=0.01': Sets the minimum support threshold.\n",
    "                                 # 'use_colnames=True': Ensures the output DataFrame uses the actual item names\n",
    "                                 # (e.g., 'Bread', 'Milk') as the elements in the itemsets, instead of column indices.\n",
    "                                 # The result is a pandas DataFrame with columns 'support' and 'itemsets'.\n",
    "                                 # 'support' contains the support value for each frequent itemset.\n",
    "                                 # 'itemsets' contains the actual set of items (as a frozenset if use_colnames=True).\n",
    "\n",
    "# Display the total number of frequent itemsets found\n",
    "print(f\"Total number of frequent itemsets (support >= {min_support_threshold}): {len(frequent_itemsets)}\")\n",
    "                                 # Prints the total count of itemsets found by Apriori that meet the minimum support.\n",
    "print(\"\\nFirst few frequent itemsets found by Apriori:\")\n",
    "                                 # Prints a header for the next output.\n",
    "print(frequent_itemsets.head(10)) # Prints the first 10 rows of the 'frequent_itemsets' DataFrame to preview results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "734877d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save directly to the Git repository folder\n",
    "maximal_df.to_csv(r'C:\\Users\\user\\-frequent-itemsets-group-BATAM\\maximal_frequent_itemsets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9739786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frequent itemsets (support >= 0.01): 465\n",
      "\n",
      "First few frequent itemsets found by Apriori:\n",
      "   support   itemsets\n",
      "0   0.1484   (Apples)\n",
      "1   0.1552  (Bananas)\n",
      "2   0.1440     (Beef)\n",
      "3   0.1574    (Bread)\n",
      "4   0.1450   (Butter)\n",
      "5   0.1466  (Carrots)\n",
      "6   0.1478   (Cereal)\n",
      "7   0.1460   (Cheese)\n",
      "8   0.1486  (Chicken)\n",
      "9   0.1524   (Coffee)\n"
     ]
    }
   ],
   "source": [
    "# --- (Student:Allan) 2. Find Frequent Itemsets using Apriori ---\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "min_support_threshold = 0.01     # Defines the minimum support threshold as a decimal (0.01 = 1%).\n",
    "                                 # An itemset must appear in at least 1% of all transactions to be considered 'frequent'.\n",
    "frequent_itemsets = apriori(df_encoded, min_support=min_support_threshold, use_colnames=True)\n",
    "                                 # Calls the Apriori algorithm from mlxtend.\n",
    "                                 # 'df_encoded': The one-hot encoded DataFrame is passed as the dataset.\n",
    "                                 # 'min_support=0.01': Sets the minimum support threshold.\n",
    "                                 # 'use_colnames=True': Ensures the output DataFrame uses the actual item names\n",
    "                                 # (e.g., 'Bread', 'Milk') as the elements in the itemsets, instead of column indices.\n",
    "                                 # The result is a pandas DataFrame with columns 'support' and 'itemsets'.\n",
    "                                 # 'support' contains the support value for each frequent itemset.\n",
    "                                 # 'itemsets' contains the actual set of items (as a frozenset if use_colnames=True).\n",
    "\n",
    "# Display the total number of frequent itemsets found\n",
    "print(f\"Total number of frequent itemsets (support >= {min_support_threshold}): {len(frequent_itemsets)}\")\n",
    "                                 # Prints the total count of itemsets found by Apriori that meet the minimum support.\n",
    "print(\"\\nFirst few frequent itemsets found by Apriori:\")\n",
    "                                 # Prints a header for the next output.\n",
    "print(frequent_itemsets.head(10)) # Prints the first 10 rows of the 'frequent_itemsets' DataFrame to preview results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
